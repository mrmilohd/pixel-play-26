{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n!pip install ultralytics -q\n!pip install git+https://github.com/openai/CLIP.git -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:29:28.187519Z","iopub.execute_input":"2026-01-07T11:29:28.188111Z","iopub.status.idle":"2026-01-07T11:29:41.619111Z","shell.execute_reply.started":"2026-01-07T11:29:28.188088Z","shell.execute_reply":"2026-01-07T11:29:41.618219Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# OPTIMIZED VIDEOPATCHCORE - SPEED MAXIMIZED\n# ============================================================\n# Key optimizations:\n# 1. Batched flip detection (process multiple frames at once)\n# 2. Cached YOLO detections\n# 3. Batched feature extraction\n# 4. Faster coreset with random pre-filtering\n# 5. Reduced I/O with smart caching\n# 6. Mixed precision throughout\n# ============================================================\n\nimport numpy as np\nimport os\nimport shutil\nfrom tqdm import tqdm\nimport random\nfrom glob import glob\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport clip\n\n# Seeds\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\nif device.type == 'cuda':\n    torch.backends.cudnn.benchmark = True\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# ============================================================\n# PATHS & CONFIG\n# ============================================================\nTRAIN_PATH = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\nTEST_PATH_ORIGINAL = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\nCLEAN_TEST_PATH = '/kaggle/working/cleaned_testing_videos'\nOUTPUT_FILE = \"submission.csv\"\n\n# VPC Config\nCLIP_LEN = 10\nCNL_POOL = 32\nOBJ_SIZE = (64, 64)\nIMG_SIZE = (224, 224)\nPOWER_N = 4\nPOOL_FOR_SM = 4\nFACTOR_X = 1.5\nFACTOR_Y = 1.2\nBATCH_SIZE = 1\nNUM_WORKERS = 4\n\n# Flip detector config\nFLIP_IMG_SIZE = 64\nFLIP_BATCH_SIZE = 128  # Increased for speed\nFLIP_EPOCHS = 5        # Reduced - usually converges fast\n\nprint(f\"Train path exists: {os.path.exists(TRAIN_PATH)}\")\nprint(f\"Test path exists: {os.path.exists(TEST_PATH_ORIGINAL)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:29:41.621079Z","iopub.execute_input":"2026-01-07T11:29:41.621792Z","iopub.status.idle":"2026-01-07T11:29:49.107402Z","shell.execute_reply.started":"2026-01-07T11:29:41.621761Z","shell.execute_reply":"2026-01-07T11:29:49.106743Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nGPU: Tesla T4\nTrain path exists: True\nTest path exists: True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================\n# FAST FLIP DETECTOR\n# ============================================================\n\nclass FlipDetectionDataset(Dataset):\n    \"\"\"Optimized flip detection dataset\"\"\"\n    def __init__(self, video_path, img_size=64, samples_per_class=2000):  # Reduced samples\n        self.img_size = img_size\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n        ])\n        \n        # Collect frame paths efficiently\n        self.frame_paths = []\n        for folder in os.listdir(video_path):\n            folder_path = os.path.join(video_path, folder)\n            if os.path.isdir(folder_path):\n                frames = [os.path.join(folder_path, f) for f in os.listdir(folder_path) \n                         if f.endswith(('.jpg', '.png'))]\n                self.frame_paths.extend(frames)\n        \n        print(f\"Total frames: {len(self.frame_paths)}\")\n        \n        # Sample\n        selected = random.sample(self.frame_paths, min(samples_per_class, len(self.frame_paths)))\n        self.samples = [(p, 0) for p in selected] + [(p, 1) for p in selected]\n        random.shuffle(self.samples)\n        print(f\"Training samples: {len(self.samples)}\")\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        img = Image.open(path).convert('L')\n        tensor = self.transform(img)\n        if label == 1:\n            tensor = TF.vflip(tensor)\n        return tensor, torch.tensor(label, dtype=torch.long)\n\n\nclass FlipDetectorCNN(nn.Module):\n    \"\"\"Lightweight flip detector\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(16, 32, 3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(32, 2)\n        )\n    \n    def forward(self, x):\n        return self.classifier(self.features(x))\n\n\ndef train_flip_detector():\n    \"\"\"Train flip detector with speed optimizations\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Training Flip Detector (Optimized)\")\n    print(\"=\"*60)\n    \n    dataset = FlipDetectionDataset(TRAIN_PATH, FLIP_IMG_SIZE, samples_per_class=2000)\n    loader = DataLoader(dataset, batch_size=FLIP_BATCH_SIZE, shuffle=True, \n                       num_workers=4, pin_memory=True)\n    \n    model = FlipDetectorCNN().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scaler = torch.amp.GradScaler('cuda')  # Mixed precision\n    \n    for epoch in range(FLIP_EPOCHS):\n        model.train()\n        correct, total = 0, 0\n        \n        pbar = tqdm(loader, desc=f'Epoch {epoch+1}/{FLIP_EPOCHS}')\n        for images, labels in pbar:\n            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            \n            with torch.amp.autocast('cuda'):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            correct += (outputs.argmax(1) == labels).sum().item()\n            total += labels.size(0)\n            pbar.set_postfix({'acc': f'{100*correct/total:.1f}%'})\n        \n        print(f\"Epoch {epoch+1}: Accuracy = {100*correct/total:.1f}%\")\n    \n    return model\n\n\ndef fix_flipped_frames_batched(model, src_path, dst_path, batch_size=64):\n    \"\"\"Batched flip detection and correction - MUCH faster\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Fixing Flipped Frames (Batched)\")\n    print(\"=\"*60)\n    \n    model.eval()\n    transform = transforms.Compose([\n        transforms.Resize((FLIP_IMG_SIZE, FLIP_IMG_SIZE)),\n        transforms.ToTensor()\n    ])\n    \n    if os.path.exists(dst_path):\n        shutil.rmtree(dst_path)\n    os.makedirs(dst_path)\n    \n    flipped_count = 0\n    total_count = 0\n    \n    video_folders = sorted([f for f in os.listdir(src_path) if os.path.isdir(os.path.join(src_path, f))])\n    \n    for folder in tqdm(video_folders, desc='Processing videos'):\n        src_folder = os.path.join(src_path, folder)\n        dst_folder = os.path.join(dst_path, folder)\n        os.makedirs(dst_folder, exist_ok=True)\n        \n        frames = sorted([f for f in os.listdir(src_folder) if f.endswith(('.jpg', '.png'))])\n        \n        # Process in batches\n        all_paths = [os.path.join(src_folder, f) for f in frames]\n        all_dst_paths = [os.path.join(dst_folder, f) for f in frames]\n        flip_flags = []\n        \n        for i in range(0, len(all_paths), batch_size):\n            batch_paths = all_paths[i:i+batch_size]\n            \n            # Load batch\n            batch_tensors = []\n            for p in batch_paths:\n                img = Image.open(p).convert('L')\n                batch_tensors.append(transform(img))\n            \n            batch = torch.stack(batch_tensors).to(device, non_blocking=True)\n            \n            with torch.no_grad(), torch.amp.autocast('cuda'):\n                outputs = model(batch)\n                probs = torch.softmax(outputs, dim=1)[:, 1]\n                is_flipped = (probs > 0.5).cpu().tolist()\n            \n            flip_flags.extend(is_flipped)\n        \n        # Save frames (copy or flip)\n        for src_p, dst_p, is_flip in zip(all_paths, all_dst_paths, flip_flags):\n            if is_flip:\n                img = Image.open(src_p)\n                img.transpose(Image.FLIP_TOP_BOTTOM).save(dst_p)\n                flipped_count += 1\n            else:\n                shutil.copy2(src_p, dst_p)\n            total_count += 1\n    \n    print(f\"Total: {total_count}, Corrected: {flipped_count} ({100*flipped_count/total_count:.1f}%)\")\n    return flipped_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:29:49.108318Z","iopub.execute_input":"2026-01-07T11:29:49.108701Z","iopub.status.idle":"2026-01-07T11:29:49.129441Z","shell.execute_reply.started":"2026-01-07T11:29:49.108677Z","shell.execute_reply":"2026-01-07T11:29:49.128691Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================================================\n# RUN FLIP DETECTION\n# ============================================================\nflip_model = train_flip_detector()\nfix_flipped_frames_batched(flip_model, TEST_PATH_ORIGINAL, CLEAN_TEST_PATH)\n\nTEST_PATH = CLEAN_TEST_PATH\nprint(f\"\\n*** TEST_PATH set to: {TEST_PATH} ***\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:29:49.130322Z","iopub.execute_input":"2026-01-07T11:29:49.130676Z","iopub.status.idle":"2026-01-07T11:32:33.328338Z","shell.execute_reply.started":"2026-01-07T11:29:49.130649Z","shell.execute_reply":"2026-01-07T11:32:33.327658Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTraining Flip Detector (Optimized)\n============================================================\nTotal frames: 9204\nTraining samples: 4000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:10<00:00,  3.19it/s, acc=51.6%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Accuracy = 51.6%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  6.43it/s, acc=69.6%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Accuracy = 69.6%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:05<00:00,  6.08it/s, acc=93.8%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Accuracy = 93.8%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:05<00:00,  6.12it/s, acc=99.9%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Accuracy = 99.9%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:05<00:00,  6.07it/s, acc=100.0%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Accuracy = 100.0%\n\n============================================================\nFixing Flipped Frames (Batched)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Processing videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [02:13<00:00,  6.33s/it]","output_type":"stream"},{"name":"stdout","text":"Total: 11706, Corrected: 1195 (10.2%)\n\n*** TEST_PATH set to: /kaggle/working/cleaned_testing_videos ***\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================\n# YOLO SETUP WITH CACHING\n# ============================================================\n\nprint(\"Loading YOLOv5...\")\nyolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True, trust_repo=True, verbose=False)\nyolo_model.eval()\nprint(\"YOLOv5L loaded.\")\n\n# Detection cache for speed\nDETECTION_CACHE = {}\n\ndef get_resized_area(areas, max_w, max_h, factor_x=FACTOR_X, factor_y=FACTOR_Y):\n    \"\"\"Resize bounding boxes with margin\"\"\"\n    if factor_x == 0 and factor_y == 0:\n        return areas\n    new_areas = []\n    for area in areas:\n        xmin_, ymin_, xmax_, ymax_ = area[:4]\n        xmin = xmin_ - (factor_x - 1) * (xmax_ - xmin_)\n        ymin = ymin_ - (factor_y - 1) * (ymax_ - ymin_)\n        xmax = xmax_ + (factor_x - 1) * (xmax_ - xmin_)\n        ymax = ymax_ + (factor_y - 1) * (ymax_ - ymin_)\n        \n        # Make square\n        x, y = xmax - xmin, ymax - ymin\n        if y > x:\n            dif = (y - x) / 2\n            xmax += dif\n            xmin -= dif\n        elif y < x:\n            dif = (x - y) / 2\n            ymax += dif\n            ymin -= dif\n        \n        xmin, ymin = max(0, xmin), max(0, ymin)\n        xmax, ymax = min(max_w, xmax), min(max_h, ymax)\n        new_areas.append([xmin, ymin, xmax, ymax])\n    return new_areas\n\n\ndef detect_objects_cached(frame_path, confidence=0.4, coi=[0]):\n    \"\"\"Cached YOLO detection\"\"\"\n    if frame_path in DETECTION_CACHE:\n        return DETECTION_CACHE[frame_path]\n    \n    with torch.no_grad():\n        results = yolo_model(frame_path)\n    \n    areas = results.xyxy[0].cpu().numpy()\n    filtered = [area for area in areas if area[4] > confidence and area[-1] in coi]\n    \n    DETECTION_CACHE[frame_path] = filtered\n    return filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:33.330418Z","iopub.execute_input":"2026-01-07T11:32:33.330865Z","iopub.status.idle":"2026-01-07T11:32:38.674607Z","shell.execute_reply.started":"2026-01-07T11:32:33.330829Z","shell.execute_reply":"2026-01-07T11:32:38.674008Z"}},"outputs":[{"name":"stdout","text":"Loading YOLOv5...\nDownloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\nCreating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"YOLOv5 ðŸš€ 2026-1-7 Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt to yolov5l.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89.3M/89.3M [00:00<00:00, 222MB/s]\n\nFusing layers... \nYOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\nAdding AutoShape... \n","output_type":"stream"},{"name":"stdout","text":"YOLOv5L loaded.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# NORMALIZATION (Preserving exact augmentation logic)\n# ============================================================\n\ndef vpc_normalize_cv2(img, target_h, target_w, augment=False):\n    \"\"\"Normalize with optional noise augmentation\"\"\"\n    img = cv2.resize(img, (target_w, target_h)).astype('float32')\n    img = img / 255.0\n\n    # Noise augmentation (40% probability) - PRESERVED\n    if augment and np.random.rand() < 0.4:\n        noise_factor = 0.1373\n        noise = np.random.uniform(-noise_factor, noise_factor, img.shape)\n        img = np.clip(img + noise, 0.0, 1.0)\n\n    # CLIP normalization\n    mean = np.array([0.48145466, 0.4578275, 0.40821073]).reshape(1, 1, 3)\n    std = np.array([0.26862954, 0.26130258, 0.27577711]).reshape(1, 1, 3)\n    img = (img - mean) / std\n\n    img = np.transpose(img, [2, 0, 1])\n    return torch.from_numpy(img).float()\n\n\ndef load_frame_cv2(path, h=224, w=224, augment=False):\n    \"\"\"Load and normalize frame\"\"\"\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return vpc_normalize_cv2(img, h, w, augment=augment)\n\n\ndef crop_and_normalize(img_cv2, bbox, obj_h=64, obj_w=64, augment=False):\n    \"\"\"Crop and normalize object\"\"\"\n    xmin, ymin, xmax, ymax = [int(x) for x in bbox]\n    cropped = img_cv2[ymin:ymax, xmin:xmax, :]\n    return vpc_normalize_cv2(cropped, obj_h, obj_w, augment=augment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:38.675387Z","iopub.execute_input":"2026-01-07T11:32:38.675764Z","iopub.status.idle":"2026-01-07T11:32:38.683145Z","shell.execute_reply.started":"2026-01-07T11:32:38.675738Z","shell.execute_reply":"2026-01-07T11:32:38.682371Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# POOLING FUNCTIONS (Optimized)\n# ============================================================\n\ndef ChannelAvgPool2d(tensor, out_channel=32):\n    \"\"\"Channel-wise average pooling\"\"\"\n    B, C, H, W = tensor.shape\n    group_size = C // out_channel\n    return tensor.view(B, out_channel, group_size, H, W).mean(dim=2)\n\n\ndef TemporalAvgPool3d(tensor, out_depths=1):\n    \"\"\"Temporal average pooling\"\"\"\n    return F.adaptive_avg_pool3d(tensor, (out_depths, tensor.shape[3], tensor.shape[4]))\n\n\ndef TemporalMaxPool3d(tensor, out_depths=1):\n    \"\"\"Temporal max pooling\"\"\"\n    return F.adaptive_max_pool3d(tensor, (out_depths, tensor.shape[3], tensor.shape[4]))\n\n\ndef SpatialAvgPool3d(tensor, out_hw=1):\n    \"\"\"Spatial average pooling\"\"\"\n    d = tensor.shape[2]\n    return F.adaptive_avg_pool3d(tensor, (d, out_hw, out_hw))\n\n\ndef SpatialMaxPool3d(tensor, out_hw=1):\n    \"\"\"Spatial max pooling\"\"\"\n    d = tensor.shape[2]\n    return F.adaptive_max_pool3d(tensor, (d, out_hw, out_hw))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:38.684284Z","iopub.execute_input":"2026-01-07T11:32:38.684500Z","iopub.status.idle":"2026-01-07T11:32:38.700948Z","shell.execute_reply.started":"2026-01-07T11:32:38.684473Z","shell.execute_reply":"2026-01-07T11:32:38.700306Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============================================================\n# VPC MODEL\n# ============================================================\n\nclass VPCModel(nn.Module):\n    def __init__(self, device=device, cnl_pool=CNL_POOL):\n        super().__init__()\n        clip_model, _ = clip.load(\"RN101\", device=device)\n        self.backbone = clip_model.visual.float()\n        self.cnl_pool = cnl_pool\n        \n        for p in self.backbone.parameters():\n            p.requires_grad = False\n        self.backbone.eval()\n\n        self.feature_maps = []\n        self.backbone.layer2[-1].register_forward_hook(self.hook)\n        self.backbone.layer3[-1].register_forward_hook(self.hook)\n        \n        self.avg = nn.AvgPool2d(3, stride=1)\n        self.resize = nn.AdaptiveAvgPool2d(28)\n        self.avg2 = nn.AvgPool2d(3, stride=1)\n        self.resize2 = nn.AdaptiveAvgPool2d(14)\n\n    def hook(self, module, input, output):\n        self.feature_maps.append(output.float())\n\n    @torch.no_grad()\n    def forward(self, x):\n        self.feature_maps = []\n        _ = self.backbone(x)\n        return self.feature_maps\n\n    def make_locally_aware_feature(self, feature_maps, len_frame):\n        \"\"\"Create locally-aware features\"\"\"\n        resized = [self.resize(self.avg(fmap)) for fmap in feature_maps]\n        locally_feat = torch.cat(resized, 1)\n        locally_feat = ChannelAvgPool2d(locally_feat, self.cnl_pool)\n        chunked = torch.chunk(locally_feat, chunks=len_frame, dim=0)\n        locally_feat = torch.stack(chunked, dim=0)\n        return locally_feat.permute(1, 2, 0, 3, 4)\n\n    def make_globally_aware_feature(self, feature_maps):\n        \"\"\"Create globally-aware features\"\"\"\n        resized = [self.resize2(self.avg2(fmap)) for fmap in feature_maps]\n        global_feat = torch.cat(resized, 1)\n        global_feat = global_feat.permute(1, 0, 2, 3).unsqueeze(0)\n        ap = nn.AvgPool3d((1, 14, 14))\n        mp = nn.MaxPool3d((1, 14, 14))\n        global_feat = ap(global_feat) + mp(global_feat)\n        return global_feat.view(1, -1, global_feat.shape[2])\n\nprint(\"Loading CLIP RN101...\")\nvpc = VPCModel().to(device)\nvpc.eval()\nprint(\"VPC Model loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:38.701877Z","iopub.execute_input":"2026-01-07T11:32:38.702219Z","iopub.status.idle":"2026-01-07T11:32:46.734976Z","shell.execute_reply.started":"2026-01-07T11:32:38.702167Z","shell.execute_reply":"2026-01-07T11:32:46.734263Z"}},"outputs":[{"name":"stdout","text":"Loading CLIP RN101...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278M/278M [00:03<00:00, 80.6MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"VPC Model loaded.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================\n# FEATURE FUNCTIONS\n# ============================================================\n\ndef MakeSpatialFeature(feature, kernel_size=(1, POOL_FOR_SM, POOL_FOR_SM)):\n    \"\"\"Create spatial features from local features\"\"\"\n    len_channel = feature.shape[1]\n    f1 = TemporalAvgPool3d(feature, 1)\n    f2 = TemporalMaxPool3d(feature, 1)\n    feature = f1 + f2\n    if kernel_size != (1, 1, 1):\n        d = feature.shape[2] // kernel_size[0]\n        h = feature.shape[3] // kernel_size[1]\n        w = feature.shape[4] // kernel_size[2]\n        feature = F.adaptive_avg_pool3d(feature, (d, h, w))\n    feature = feature.squeeze(2).permute(0, 2, 3, 1)\n    return feature.reshape(-1, len_channel)\n\n\ndef MakeTemporalFeature(feature, mode='train'):\n    \"\"\"Create temporal (motion) features\"\"\"\n    len_channel = feature.shape[1]\n    len_frame = feature.shape[2]\n    resolution = feature.shape[3]\n    \n    # Compute frame differences\n    temporal = feature[:, :, 1:] - feature[:, :, :-1]\n    temporal = torch.abs(temporal)\n    \n    if mode == 'train':\n        temporal = SpatialAvgPool3d(temporal, 1)\n    else:\n        temporal = SpatialMaxPool3d(temporal, 1)\n    \n    temporal = temporal.permute(0, 2, 1, 3, 4)\n    return temporal.reshape(-1, len_channel)\n\n\ndef MakeHighlevelFeature(feature):\n    \"\"\"Create high-level semantic features with temporal pyramid pooling\"\"\"\n    pool = nn.MaxPool1d(2, stride=1)\n    channels = feature.shape[1]\n    f0 = feature\n    f1 = pool(f0)\n    f2 = pool(f1)\n    out = F.adaptive_max_pool1d(f2, output_size=f1.shape[2])\n    out = out + f1\n    out = F.adaptive_max_pool1d(out, output_size=f0.shape[2])\n    out = out + f0\n    return out.view(1, channels, -1).permute(0, 2, 1).squeeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:46.735927Z","iopub.execute_input":"2026-01-07T11:32:46.736238Z","iopub.status.idle":"2026-01-07T11:32:46.744432Z","shell.execute_reply.started":"2026-01-07T11:32:46.736199Z","shell.execute_reply":"2026-01-07T11:32:46.743617Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ============================================================\n# FAST CORESET SUBSAMPLING\n# ============================================================\n\ndef get_coreset_fast(memory_bank, l=1000, device='cuda'):\n    \"\"\"Fast coreset with random pre-filtering\"\"\"\n    if len(memory_bank) <= l:\n        return memory_bank\n    \n    # Random pre-filter if too large (speed optimization)\n    if len(memory_bank) > 15000:\n        indices = torch.randperm(len(memory_bank))[:15000]\n        memory_bank = memory_bank[indices]\n    \n    memory_bank = memory_bank.to(device)\n    n = len(memory_bank)\n    \n    # Greedy coreset selection\n    selected = [random.randint(0, n-1)]\n    min_distances = torch.full((n,), float('inf'), device=device)\n    \n    for _ in tqdm(range(l - 1), desc='Coreset', leave=False):\n        # Update distances\n        last = memory_bank[selected[-1]:selected[-1]+1]\n        dists = torch.linalg.norm(memory_bank - last, dim=1)\n        min_distances = torch.minimum(dists, min_distances)\n        min_distances[selected[-1]] = -1\n        \n        # Select farthest point\n        selected.append(torch.argmax(min_distances).item())\n    \n    return memory_bank[selected].cpu()\n\n\ndef predict(features, memory_bank, power_n=POWER_N):\n    \"\"\"Compute anomaly score\"\"\"\n    distances = torch.cdist(features, memory_bank, p=2.0)\n    dist_score, _ = torch.min(distances, dim=1)\n    s_star = torch.max(dist_score)\n    return s_star ** power_n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:46.745501Z","iopub.execute_input":"2026-01-07T11:32:46.745853Z","iopub.status.idle":"2026-01-07T11:32:46.759991Z","shell.execute_reply.started":"2026-01-07T11:32:46.745797Z","shell.execute_reply":"2026-01-07T11:32:46.759312Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ============================================================\n# DATASET & OBJECT EXTRACTION\n# ============================================================\n\nclass ClipDataset(Dataset):\n    def __init__(self, root_dir, clip_len=10):\n        self.clip_len = clip_len\n        self.clips = []\n        if not os.path.exists(root_dir):\n            return\n        for vid in sorted(os.listdir(root_dir)):\n            vid_path = os.path.join(root_dir, vid)\n            if not os.path.isdir(vid_path):\n                continue\n            frames = sorted(glob(os.path.join(vid_path, \"*.jpg\")))\n            for i in range(0, len(frames) - clip_len + 1, clip_len):\n                self.clips.append(frames[i:i + clip_len])\n\n    def __len__(self):\n        return len(self.clips)\n\n    def __getitem__(self, idx):\n        return self.clips[idx]\n\n\ndef extract_object_tubes(frame_paths, confidence=0.4, augment=False):\n    \"\"\"Extract object tubes from frames\"\"\"\n    T = len(frame_paths)\n    mid_idx = T // 2 if T >= 4 else -1\n    \n    # Detect objects on middle frame (cached)\n    areas = detect_objects_cached(frame_paths[mid_idx], confidence=confidence, coi=[0])\n    if not areas:\n        return None\n    \n    img = cv2.imread(frame_paths[0])\n    H, W = img.shape[:2]\n    areas = get_resized_area(areas, max_w=W, max_h=H, factor_x=FACTOR_X, factor_y=FACTOR_Y)\n    \n    if not areas:\n        return None\n    \n    # Load all frames once\n    frames = [cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB) for p in frame_paths]\n    N_obj = len(areas)\n    tubes = torch.zeros((N_obj, 3, T, OBJ_SIZE[0], OBJ_SIZE[1]))\n    \n    for obj_idx, bbox in enumerate(areas):\n        for t, frame in enumerate(frames):\n            tubes[obj_idx, :, t, :, :] = crop_and_normalize(frame, bbox, OBJ_SIZE[0], OBJ_SIZE[1], augment=augment)\n    \n    return tubes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:46.760823Z","iopub.execute_input":"2026-01-07T11:32:46.761105Z","iopub.status.idle":"2026-01-07T11:32:46.774337Z","shell.execute_reply.started":"2026-01-07T11:32:46.761075Z","shell.execute_reply":"2026-01-07T11:32:46.773876Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ============================================================\n# TRAINING PIPELINE\n# ============================================================\n\ndef train_pipeline():\n    \"\"\"Build memory banks from training data\"\"\"\n    if os.path.exists(\"SM_prime.pt\"):\n        print(\"Memory banks exist. Skipping training.\")\n        return\n\n    print(f\"\\n--- Training on {TRAIN_PATH} ---\")\n    dataset = ClipDataset(TRAIN_PATH, CLIP_LEN)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n                       shuffle=False, pin_memory=True)\n    \n    SM, TM, GM = [], [], []\n\n    for idx, batch in enumerate(tqdm(loader, desc='Training')):\n        paths = [batch[i][0] for i in range(CLIP_LEN)]\n        \n        # Local stream\n        tubes = extract_object_tubes(paths, confidence=0.4, augment=True)\n        if tubes is not None:\n            N, C, T, H, W = tubes.shape\n            x = tubes.permute(0, 2, 1, 3, 4).reshape(N * T, C, H, W)\n            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False).to(device)\n            \n            with torch.amp.autocast('cuda'):\n                fmaps = vpc(x)\n                local_feat = vpc.make_locally_aware_feature(fmaps, T)\n                SM.append(MakeSpatialFeature(local_feat).cpu())\n                TM.append(MakeTemporalFeature(local_feat, mode='train').cpu())\n        \n        # Global stream\n        global_imgs = torch.stack([load_frame_cv2(p, 224, 224, augment=True) for p in paths]).to(device)\n        with torch.amp.autocast('cuda'):\n            gfmaps = vpc(global_imgs)\n            global_feat = vpc.make_globally_aware_feature(gfmaps)\n            GM.append(MakeHighlevelFeature(global_feat).cpu())\n\n    # Build and save memory banks\n    if SM:\n        SM, TM = torch.cat(SM), torch.cat(TM)\n        print(f\"\\nReducing Spatial: {len(SM)} -> 3000\")\n        torch.save(get_coreset_fast(SM, 3000), \"SM_prime.pt\")\n        print(f\"Reducing Temporal: {len(TM)} -> 2000\")\n        torch.save(get_coreset_fast(TM, 2000), \"TM_prime.pt\")\n    \n    if GM:\n        GM = torch.cat(GM)\n        print(f\"Reducing Global: {len(GM)} -> 1000\")\n        torch.save(get_coreset_fast(GM, 1000), \"HSM_prime.pt\")\n    \n    print(\"Training Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:46.775249Z","iopub.execute_input":"2026-01-07T11:32:46.775917Z","iopub.status.idle":"2026-01-07T11:32:46.789079Z","shell.execute_reply.started":"2026-01-07T11:32:46.775890Z","shell.execute_reply":"2026-01-07T11:32:46.788507Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ============================================================\n# INFERENCE PIPELINE\n# ============================================================\n\ndef inference_pipeline():\n    \"\"\"Compute anomaly scores for test data\"\"\"\n    print(f\"\\n--- Inference on {TEST_PATH} ---\")\n    \n    SMp = torch.load(\"SM_prime.pt\").to(device)\n    TMp = torch.load(\"TM_prime.pt\").to(device)\n    HSMp = torch.load(\"HSM_prime.pt\").to(device)\n\n    dataset = ClipDataset(TEST_PATH, CLIP_LEN)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n                       shuffle=False, pin_memory=True)\n    \n    scores = []\n    \n    for idx, batch in enumerate(tqdm(loader, desc='Inference')):\n        paths = [batch[i][0] for i in range(CLIP_LEN)]\n        \n        LAS = 0.0\n        tubes = extract_object_tubes(paths, confidence=0.4, augment=False)\n        \n        if tubes is not None:\n            N, C, T, H, W = tubes.shape\n            x = tubes.permute(0, 2, 1, 3, 4).reshape(N * T, C, H, W)\n            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False).to(device)\n            \n            with torch.amp.autocast('cuda'):\n                fmaps = vpc(x)\n                local_feat = vpc.make_locally_aware_feature(fmaps, T)\n                sp = MakeSpatialFeature(local_feat).to(device)\n                tm = MakeTemporalFeature(local_feat, mode='test').to(device)\n                ds = predict(sp, SMp, POWER_N)\n                dt = predict(tm, TMp, POWER_N)\n                LAS = 0.7 * ds + 0.3 * dt\n\n        # Global stream (augment=True preserved from original)\n        global_imgs = torch.stack([load_frame_cv2(p, 224, 224, augment=True) for p in paths]).to(device)\n        with torch.amp.autocast('cuda'):\n            gfmaps = vpc(global_imgs)\n            global_feat = vpc.make_globally_aware_feature(gfmaps)\n            highlevel = MakeHighlevelFeature(global_feat).to(device)\n            dg = predict(highlevel, HSMp, POWER_N)\n\n        final = (0.7 * LAS + 0.3 * dg).cpu().item()\n        scores.append(final)\n\n    # Normalize scores\n    scores = torch.tensor(scores)\n    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)\n    torch.save(scores, \"frame_scores.pt\")\n    print(\"Inference Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:46.790014Z","iopub.execute_input":"2026-01-07T11:32:46.790275Z","iopub.status.idle":"2026-01-07T11:32:46.804588Z","shell.execute_reply.started":"2026-01-07T11:32:46.790252Z","shell.execute_reply":"2026-01-07T11:32:46.804048Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ============================================================\n# GENERATE CSV\n# ============================================================\n\ndef generate_csv():\n    \"\"\"Generate submission CSV\"\"\"\n    print(\"\\n--- Generating Submission CSV ---\")\n    if not os.path.exists(\"frame_scores.pt\"):\n        print(\"No scores found!\")\n        return\n    \n    clip_scores = torch.load(\"frame_scores.pt\").numpy()\n    video_folders = sorted(os.listdir(TEST_PATH))\n    \n    submission_rows = []\n    score_ptr = 0\n    \n    for vid_folder in video_folders:\n        vid_path = os.path.join(TEST_PATH, vid_folder)\n        if not os.path.isdir(vid_path):\n            continue\n        \n        frames = sorted(glob(os.path.join(vid_path, \"*.jpg\")))\n        num_frames = len(frames)\n        num_clips = num_frames // CLIP_LEN\n        \n        if num_clips == 0:\n            vid_scores = [0.0] * num_frames\n        else:\n            c_scores = clip_scores[score_ptr:score_ptr + num_clips]\n            score_ptr += num_clips\n            vid_scores = np.repeat(c_scores, CLIP_LEN)\n            remaining = num_frames - len(vid_scores)\n            if remaining > 0:\n                vid_scores = np.concatenate([vid_scores, [c_scores[-1]] * remaining])\n\n        try:\n            vid_id = int(vid_folder)\n        except ValueError:\n            vid_id = vid_folder\n\n        for i, f_path in enumerate(frames):\n            f_name = os.path.basename(f_path).split('.')[0]\n            f_num = int(\"\".join(filter(str.isdigit, f_name)))\n            row_id = f\"{vid_id}_{f_num}\"\n            submission_rows.append([row_id, vid_scores[i]])\n\n    df = pd.DataFrame(submission_rows, columns=['Id', 'Predicted'])\n    df.to_csv(OUTPUT_FILE, index=False)\n    print(f\"Saved {OUTPUT_FILE} with {len(df)} rows.\")\n    print(df['Predicted'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:46.806763Z","iopub.execute_input":"2026-01-07T11:32:46.807051Z","iopub.status.idle":"2026-01-07T11:32:46.818730Z","shell.execute_reply.started":"2026-01-07T11:32:46.807028Z","shell.execute_reply":"2026-01-07T11:32:46.818049Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ============================================================\n# MAIN - RUN EVERYTHING\n# ============================================================\n\nif __name__ == \"__main__\":\n    print(\"=\"*60)\n    print(\"VIDEOPATCHCORE - OPTIMIZED\")\n    print(\"=\"*60)\n    \n    train_pipeline()\n    inference_pipeline()\n    generate_csv()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"COMPLETE!\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T11:32:46.819523Z","iopub.execute_input":"2026-01-07T11:32:46.819867Z","iopub.status.idle":"2026-01-07T11:48:56.981001Z","shell.execute_reply.started":"2026-01-07T11:32:46.819834Z","shell.execute_reply":"2026-01-07T11:48:56.979840Z"}},"outputs":[{"name":"stdout","text":"============================================================\nVIDEOPATCHCORE - OPTIMIZED\n============================================================\n\n--- Training on /kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 913/913 [08:56<00:00,  1.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nReducing Spatial: 359758 -> 3000\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Reducing Temporal: 66078 -> 2000\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Reducing Global: 9130 -> 1000\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Training Complete.\n\n--- Inference on /kaggle/working/cleaned_testing_videos ---\n","output_type":"stream"},{"name":"stderr","text":"Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1159/1159 [07:12<00:00,  2.68it/s]","output_type":"stream"},{"name":"stdout","text":"Inference Complete.\n\n--- Generating Submission CSV ---\nSaved submission.csv with 11706 rows.\ncount    11706.000000\nmean         0.147162\nstd          0.136209\nmin          0.000000\n25%          0.041920\n50%          0.088116\n75%          0.229660\nmax          1.000000\nName: Predicted, dtype: float64\n\n============================================================\nCOMPLETE!\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15}]}